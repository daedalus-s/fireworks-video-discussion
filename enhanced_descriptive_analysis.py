"""
Enhanced Video Analysis System with Highly Descriptive Analysis
Generates detailed, comprehensive descriptions for better search and understanding
"""

import os
import asyncio
import json
import logging
import random
from pathlib import Path
from typing import List, Dict, Any, Optional
from dataclasses import dataclass, asdict
from datetime import datetime
import time

from video_processor import VideoProcessor, SubtitleProcessor, VideoFrame, SubtitleSegment
from fireworks_client import FireworksClient, AnalysisResult
from simple_api_manager import SimpleAPIManager
from api_manager import OptimizedAPIManager, ParallelAPIProcessor

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

@dataclass
class EnhancedVideoAnalysisResult:
    """Enhanced video analysis results with more detailed information"""
    video_path: str
    frame_count: int
    subtitle_count: int
    frame_analyses: List[Dict]
    subtitle_analyses: List[Dict]
    overall_analysis: str
    scene_breakdown: List[Dict]
    visual_elements: Dict[str, Any]
    content_categories: List[str]
    processing_time: float
    total_cost: float
    timestamp: str

class EnhancedVideoAnalysisSystem:
    """Enhanced system with highly descriptive analysis capabilities"""
    
    def __init__(self, api_key: Optional[str] = None):
        """Initialize the enhanced video analysis system"""
        self.video_processor = VideoProcessor(output_dir="frames")
        self.subtitle_processor = SubtitleProcessor()
        self.fireworks_client = FireworksClient(api_key)
        self.api_manager = OptimizedAPIManager()
        logger.info("‚úÖ Enhanced Video Analysis System initialized")
    
    async def analyze_video_descriptive(self,
                                      video_path: str,
                                      subtitle_path: Optional[str] = None,
                                      max_frames: int = 10,
                                      fps_extract: float = 0.2,
                                      analysis_depth: str = "comprehensive") -> EnhancedVideoAnalysisResult:
        """
        Perform highly descriptive video analysis
        
        Args:
            video_path: Path to video file
            subtitle_path: Optional path to subtitle file
            max_frames: Maximum frames to analyze
            fps_extract: Frames per second to extract
            analysis_depth: "basic", "detailed", or "comprehensive"
            
        Returns:
            EnhancedVideoAnalysisResult with detailed analysis
        """
        start_time = time.time()
        logger.info(f"Starting enhanced descriptive analysis for: {video_path}")
        logger.info(f"Analysis depth: {analysis_depth}")
        
        # Step 1: Extract frames
        logger.info("Step 1: Extracting frames...")
        frames = self.video_processor.extract_frames(
            video_path=video_path,
            fps_extract=fps_extract,
            max_frames=max_frames
        )
        logger.info(f"‚úÖ Extracted {len(frames)} frames")
        
        # Step 2: Process subtitles if available
        subtitles = []
        if subtitle_path and os.path.exists(subtitle_path):
            logger.info("Step 2: Processing subtitles...")
            if subtitle_path.endswith('.vtt'):
                subtitles = self.subtitle_processor.parse_vtt(subtitle_path)
            elif subtitle_path.endswith('.srt'):
                subtitles = self.subtitle_processor.parse_srt(subtitle_path)
            logger.info(f"‚úÖ Processed {len(subtitles)} subtitle segments")
        else:
            logger.info("Step 2: No subtitles provided")
        
        # Step 3: Perform enhanced frame analysis
        logger.info("Step 3: Performing enhanced descriptive frame analysis...")
        frame_analyses = await self._analyze_frames_descriptive(frames, analysis_depth)
        
        # Step 4: Analyze subtitles with enhanced detail
        subtitle_analyses = []
        if subtitles:
            logger.info("Step 4: Performing enhanced subtitle analysis...")
            subtitle_analyses = await self._analyze_subtitles_descriptive(subtitles, analysis_depth)
        else:
            logger.info("Step 4: Skipping subtitle analysis (no subtitles)")
        
        # Step 5: Generate scene breakdown
        logger.info("Step 5: Generating scene breakdown...")
        scene_breakdown = await self._generate_scene_breakdown(frames, frame_analyses, subtitles, subtitle_analyses)
        
        # Step 6: Extract visual elements
        logger.info("Step 6: Extracting visual elements...")
        visual_elements = self._extract_visual_elements(frame_analyses)
        
        # Step 7: Categorize content
        logger.info("Step 7: Categorizing content...")
        content_categories = self._categorize_content(frame_analyses, subtitle_analyses)
        
        # Step 8: Generate comprehensive overall analysis
        logger.info("Step 8: Generating comprehensive overall analysis...")
        overall_analysis = await self._generate_comprehensive_analysis(
            frames=frames,
            frame_analyses=frame_analyses,
            subtitles=subtitles,
            subtitle_analyses=subtitle_analyses,
            scene_breakdown=scene_breakdown,
            visual_elements=visual_elements,
            content_categories=content_categories
        )
        
        # Calculate processing time and cost
        processing_time = time.time() - start_time
        usage_summary = self.fireworks_client.get_usage_summary()
        
        # Compile enhanced results
        result = EnhancedVideoAnalysisResult(
            video_path=video_path,
            frame_count=len(frames),
            subtitle_count=len(subtitles),
            frame_analyses=frame_analyses,
            subtitle_analyses=subtitle_analyses,
            overall_analysis=overall_analysis,
            scene_breakdown=scene_breakdown,
            visual_elements=visual_elements,
            content_categories=content_categories,
            processing_time=processing_time,
            total_cost=usage_summary['total_cost_usd'],
            timestamp=datetime.now().isoformat()
        )
        
        logger.info(f"‚úÖ Enhanced analysis complete in {processing_time:.2f} seconds")
        logger.info(f"üí∞ Total cost: ${usage_summary['total_cost_usd']:.4f}")
        
        return result
    
    async def _analyze_frames_descriptive(self, frames: List[VideoFrame], analysis_depth: str) -> List[Dict]:
        """OPTIMIZED: 10x faster frame analysis"""
        analyses = []
        
        # Remove the slow delays!
        for i, frame in enumerate(frames):
            logger.info(f"  Analyzing frame {i+1}/{len(frames)} at {frame.timestamp:.2f}s...")
            
            # Use optimized API manager instead of sleep
            result = await self.api_manager.call_with_retry(
                self.fireworks_client.analyze_frame,
                api_type='vision',
                priority=2 if i == 0 else 1,  # First frame gets priority
                base64_image=frame.base64_image,
                prompt=self._get_comprehensive_frame_prompt(frame),
                max_tokens=800
            )
            
            # Parse and add result
            parsed_analysis = self._parse_frame_analysis(result.content, frame.timestamp)
            analyses.append({
                "frame_number": frame.frame_number,
                "timestamp": frame.timestamp,
                "analysis": result.content,
                "parsed_analysis": parsed_analysis,
                "tokens_used": result.tokens_used,
                "cost": result.cost,
                "analysis_depth": analysis_depth
            })
        
        return analyses
    
    def _get_comprehensive_frame_prompt(self, frame: VideoFrame) -> str:
        """Generate comprehensive analysis prompt for maximum detail"""
        return f"""Provide an extremely detailed and comprehensive analysis of this video frame at timestamp {frame.timestamp:.2f} seconds.

STRUCTURE YOUR RESPONSE WITH THESE SECTIONS:

**VISUAL COMPOSITION & CINEMATOGRAPHY:**
- Camera angle, shot type (close-up, medium, wide, etc.)
- Framing, composition rules (rule of thirds, leading lines, symmetry)
- Depth of field, focus points, bokeh effects
- Camera movement indicators (static, pan, tilt, zoom)
- Aspect ratio and visual format observations

**LIGHTING & COLOR ANALYSIS:**
- Lighting setup (natural, artificial, mixed)
- Light direction (front, side, back, top)
- Color palette and dominant colors
- Color temperature (warm, cool, neutral)
- Contrast levels and shadow details
- Mood created by lighting choices

**SUBJECTS & OBJECTS (Detailed Inventory):**
- Every person visible (clothing, posture, facial expressions, activities)
- All significant objects (furniture, vehicles, tools, technology)
- Architecture and environmental elements
- Props and their potential significance
- Brands, logos, or text visible

**SETTING & ENVIRONMENT:**
- Specific location type (indoor/outdoor specifics)
- Time of day indicators
- Weather conditions (if visible)
- Architectural style
- Geographical or cultural indicators
- Background details and context clues

**ACTION & MOVEMENT:**
- Specific actions being performed by people
- Object movement or changes
- Implied motion or direction
- Body language and gestures
- Interaction between subjects
- Energy level of the scene

**AUDIO-VISUAL SYNCHRONIZATION:**
- Relationship to dialogue (if any subtitles visible)
- Sound source indicators visible in frame
- Musical or audio cues suggested by visuals

**TECHNICAL QUALITY:**
- Image sharpness and focus quality
- Exposure quality (over/under exposed areas)
- Any technical artifacts or issues
- Production value indicators

**EMOTIONAL & NARRATIVE CONTEXT:**
- Mood and atmosphere
- Emotional tone conveyed
- Story implications
- Character relationships suggested
- Tension or conflict indicators
- Genre conventions visible

**TEXT & GRAPHICS:**
- Any visible text (signs, screens, documents)
- Graphics or overlays
- Captions or titles
- User interface elements

**CULTURAL & CONTEXTUAL ELEMENTS:**
- Cultural references or indicators
- Historical period clues
- Social context implications
- Economic status indicators

Be extremely specific and observant. Include details that might be easily missed. Describe not just what you see, but what it implies about the story, characters, and production."""

    def _get_detailed_frame_prompt(self, frame: VideoFrame) -> str:
        """Generate detailed analysis prompt"""
        return f"""Provide a detailed analysis of this video frame at timestamp {frame.timestamp:.2f} seconds.

Analyze in detail:

**VISUAL ELEMENTS:**
- Composition and framing techniques
- Lighting setup and color scheme
- Camera positioning and shot type

**CONTENT INVENTORY:**
- All people present (appearance, actions, expressions)
- Objects and props (detailed descriptions)
- Environmental details and setting

**TECHNICAL ASPECTS:**
- Image quality and production value
- Any text or graphics visible
- Audio-visual synchronization clues

**NARRATIVE CONTEXT:**
- Scene mood and atmosphere
- Character relationships and interactions
- Story implications and genre indicators

**SPECIFIC DETAILS:**
- Clothing, hairstyles, and personal items
- Technology and time period indicators
- Cultural or geographical context clues

Be thorough and specific in your observations."""

    def _get_basic_frame_prompt(self, frame: VideoFrame) -> str:
        """Generate basic analysis prompt"""
        return f"""Analyze this video frame at timestamp {frame.timestamp:.2f} seconds.

Describe:
1. Main subjects or objects visible
2. Actions or events happening
3. Setting or environment
4. Visual style and mood
5. Any text or important details

Be concise but specific."""

    def _parse_frame_analysis(self, analysis_text: str, timestamp: float) -> Dict[str, Any]:
        """Parse the frame analysis into structured data"""
        parsed = {
            "timestamp": timestamp,
            "subjects": [],
            "objects": [],
            "setting": "",
            "actions": [],
            "mood": "",
            "colors": [],
            "lighting": "",
            "camera_work": "",
            "text_elements": [],
            "technical_quality": "",
            "narrative_context": ""
        }
        
        # Simple keyword extraction for better searchability
        analysis_lower = analysis_text.lower()
        
        # Extract subjects (people-related keywords)
        people_keywords = ["person", "man", "woman", "child", "people", "crowd", "figure", "character"]
        for keyword in people_keywords:
            if keyword in analysis_lower:
                parsed["subjects"].append(keyword)
        
        # Extract objects
        object_keywords = ["car", "building", "tree", "phone", "computer", "table", "chair", "dog", "cat"]
        for keyword in object_keywords:
            if keyword in analysis_lower:
                parsed["objects"].append(keyword)
        
        # Extract colors
        color_keywords = ["red", "blue", "green", "yellow", "black", "white", "brown", "purple", "orange", "pink"]
        for color in color_keywords:
            if color in analysis_lower:
                parsed["colors"].append(color)
        
        # Extract mood/atmosphere
        mood_keywords = ["happy", "sad", "dramatic", "peaceful", "tense", "bright", "dark", "energetic", "calm"]
        for mood in mood_keywords:
            if mood in analysis_lower:
                parsed["mood"] = mood
                break
        
        return parsed
    
    async def _analyze_subtitles_descriptive(self, subtitles: List[SubtitleSegment], analysis_depth: str) -> List[Dict]:
        """Perform enhanced subtitle analysis"""
        analyses = []
        
        # Group subtitles into chunks for context
        chunk_size = 3 if analysis_depth == "comprehensive" else 5
        
        for i in range(0, len(subtitles), chunk_size):
            chunk = subtitles[i:i+chunk_size]
            
            # Combine subtitle text
            combined_text = "\n".join([
                f"[{s.start_time:.1f}s - {s.end_time:.1f}s]: {s.text}"
                for s in chunk
            ])
            
            logger.info(f"  Analyzing subtitle chunk {i//chunk_size + 1} with {analysis_depth} detail...")
            
            prompt = self._get_subtitle_analysis_prompt(combined_text, analysis_depth)
            
            # Rate limiting with retries
            max_retries = 3
            for attempt in range(max_retries):
                try:
                    delay = 4 + random.uniform(0, 2)
                    if attempt > 0:
                        delay *= (2 ** attempt)
                    
                    await self.api_manager.acquire() 
                    
                    result = await self.fireworks_client.analyze_text(
                        text=prompt,
                        model_type="gpt_oss",
                        max_tokens=400 if analysis_depth == "comprehensive" else 250
                    )
                    
                    analyses.append({
                        "subtitle_range": f"{chunk[0].start_time:.1f}s - {chunk[-1].end_time:.1f}s",
                        "text_analyzed": combined_text,
                        "analysis": result.content,
                        "tokens_used": result.tokens_used,
                        "cost": result.cost,
                        "analysis_depth": analysis_depth
                    })
                    break
                    
                except Exception as e:
                    if "429" in str(e) and attempt < max_retries - 1:
                        wait_time = 20 * (2 ** attempt)
                        logger.warning(f"‚è±Ô∏è Subtitle analysis rate limited, waiting {wait_time}s...")
                        await self.api_manager.acquire() 
                    elif attempt == max_retries - 1:
                        logger.warning(f"‚ùå Subtitle analysis failed after {max_retries} attempts")
                        analyses.append({
                            "subtitle_range": f"{chunk[0].start_time:.1f}s - {chunk[-1].end_time:.1f}s",
                            "text_analyzed": combined_text,
                            "analysis": "Enhanced subtitle analysis failed due to rate limiting",
                            "tokens_used": 0,
                            "cost": 0,
                            "analysis_depth": analysis_depth
                        })
                        break
                    else:
                        raise e
        
        return analyses
    
    def _get_subtitle_analysis_prompt(self, combined_text: str, analysis_depth: str) -> str:
        """Generate subtitle analysis prompt based on depth"""
        base_prompt = f"""Analyze this dialogue/text from the video with {analysis_depth} detail:

{combined_text}

"""
        
        if analysis_depth == "comprehensive":
            return base_prompt + """Provide comprehensive analysis covering:

**CONTENT ANALYSIS:**
- Main topics and themes discussed
- Key information and facts presented
- Emotional content and sentiment

**SPEAKER CHARACTERISTICS:**
- Number of speakers (if identifiable)
- Speaking style and tone
- Formality level and register
- Personality indicators

**DIALOGUE DYNAMICS:**
- Conversation flow and turn-taking
- Interruptions or overlaps
- Power dynamics between speakers
- Conflict or agreement indicators

**LINGUISTIC FEATURES:**
- Vocabulary complexity
- Sentence structure patterns
- Regional or cultural language markers
- Technical or specialized terminology

**CONTEXTUAL IMPLICATIONS:**
- Setting or situation implied by dialogue
- Relationship between speakers
- Time period or cultural context
- Genre conventions reflected

**NARRATIVE FUNCTION:**
- Plot advancement
- Character development
- Exposition or background information
- Foreshadowing or tension building

Be thorough and analytical in your assessment."""
        
        elif analysis_depth == "detailed":
            return base_prompt + """Analyze in detail:
1. Main topics and themes
2. Emotional tone and sentiment
3. Speaker characteristics and relationships
4. Key information conveyed
5. Contextual implications
6. Narrative significance

Provide specific observations and insights."""
        
        else:  # basic
            return base_prompt + """Identify:
1. Main topics discussed
2. Tone and emotion
3. Key information conveyed
4. Speaker characteristics (if apparent)

Be concise but insightful."""

    async def _generate_scene_breakdown(self, frames: List[VideoFrame], frame_analyses: List[Dict], 
                                      subtitles: List[SubtitleSegment], subtitle_analyses: List[Dict]) -> List[Dict]:
        """Generate a breakdown of scenes and segments"""
        scenes = []
        
        # Simple scene detection based on visual and dialogue changes
        scene_length = max(1, len(frames) // 4)  # Divide video into segments
        
        for i in range(0, len(frames), scene_length):
            scene_frames = frames[i:i + scene_length]
            scene_analyses = frame_analyses[i:i + scene_length]
            
            if not scene_frames:
                continue
            
            start_time = scene_frames[0].timestamp
            end_time = scene_frames[-1].timestamp
            
            # Find relevant subtitles for this scene
            scene_subtitles = [
                s for s in subtitles 
                if start_time <= s.start_time <= end_time or start_time <= s.end_time <= end_time
            ]
            
            # Analyze this scene segment
            scene_summary = await self._analyze_scene_segment(scene_analyses, scene_subtitles)
            
            scenes.append({
                "scene_number": len(scenes) + 1,
                "start_time": start_time,
                "end_time": end_time,
                "duration": end_time - start_time,
                "frame_count": len(scene_frames),
                "subtitle_count": len(scene_subtitles),
                "summary": scene_summary,
                "key_visual_elements": self._extract_scene_elements(scene_analyses),
                "dialogue_summary": self._summarize_scene_dialogue(scene_subtitles)
            })
        
        return scenes
    
    async def _analyze_scene_segment(self, scene_analyses: List[Dict], scene_subtitles: List[SubtitleSegment]) -> str:
        """Analyze a specific scene segment"""
        if not scene_analyses:
            return "No analysis available for this scene segment"
        
        # Combine visual and dialogue information
        visual_info = "\n".join([
            f"Frame {sa.get('timestamp', 0):.1f}s: {sa.get('analysis', '')[:200]}..."
            for sa in scene_analyses
            if sa.get('analysis') and not 'failed' in sa.get('analysis', '').lower()
        ])
        
        dialogue_info = "\n".join([
            f"[{s.start_time:.1f}s]: {s.text}"
            for s in scene_subtitles[:5]  # Limit to avoid token limits
        ])
        
        prompt = f"""Analyze this scene segment and provide a concise summary:

VISUAL CONTENT:
{visual_info}

DIALOGUE:
{dialogue_info}

Provide a 2-3 sentence summary of what happens in this scene segment, focusing on:
1. Main action or events
2. Key dialogue points
3. Overall scene purpose or significance"""
        
        try:
            await self.api_manager.acquire() 
            result = await self.fireworks_client.analyze_text(
                text=prompt,
                model_type="small",  # Use smaller model for scene summaries
                max_tokens=150
            )
            return result.content
        except:
            return f"Scene segment with {len(scene_analyses)} frames and {len(scene_subtitles)} dialogue segments"
    
    def _extract_scene_elements(self, scene_analyses: List[Dict]) -> List[str]:
        """Extract key visual elements from scene analyses"""
        elements = set()
        
        for analysis in scene_analyses:
            if 'parsed_analysis' in analysis:
                parsed = analysis['parsed_analysis']
                elements.update(parsed.get('subjects', []))
                elements.update(parsed.get('objects', []))
                elements.update(parsed.get('colors', []))
        
        return list(elements)[:10]  # Top 10 elements
    
    def _summarize_scene_dialogue(self, scene_subtitles: List[SubtitleSegment]) -> str:
        """Summarize dialogue in a scene"""
        if not scene_subtitles:
            return "No dialogue in this scene"
        
        # Combine all dialogue
        all_text = " ".join([s.text for s in scene_subtitles])
        
        if len(all_text) > 200:
            return all_text[:200] + "..."
        return all_text
    
    def _extract_visual_elements(self, frame_analyses: List[Dict]) -> Dict[str, Any]:
        """Extract and categorize visual elements across all frames"""
        visual_elements = {
            "dominant_subjects": [],
            "common_objects": [],
            "color_palette": [],
            "settings": [],
            "moods": [],
            "camera_techniques": [],
            "lighting_styles": [],
            "text_elements": []
        }
        
        # Count occurrences of elements
        subject_counts = {}
        object_counts = {}
        color_counts = {}
        mood_counts = {}
        
        for analysis in frame_analyses:
            if 'parsed_analysis' in analysis:
                parsed = analysis['parsed_analysis']
                
                # Count subjects
                for subject in parsed.get('subjects', []):
                    subject_counts[subject] = subject_counts.get(subject, 0) + 1
                
                # Count objects
                for obj in parsed.get('objects', []):
                    object_counts[obj] = object_counts.get(obj, 0) + 1
                
                # Count colors
                for color in parsed.get('colors', []):
                    color_counts[color] = color_counts.get(color, 0) + 1
                
                # Count moods
                mood = parsed.get('mood', '')
                if mood:
                    mood_counts[mood] = mood_counts.get(mood, 0) + 1
        
        # Get top elements
        visual_elements["dominant_subjects"] = sorted(subject_counts.items(), key=lambda x: x[1], reverse=True)[:5]
        visual_elements["common_objects"] = sorted(object_counts.items(), key=lambda x: x[1], reverse=True)[:5]
        visual_elements["color_palette"] = sorted(color_counts.items(), key=lambda x: x[1], reverse=True)[:5]
        visual_elements["moods"] = sorted(mood_counts.items(), key=lambda x: x[1], reverse=True)[:3]
        
        return visual_elements
    
    def _categorize_content(self, frame_analyses: List[Dict], subtitle_analyses: List[Dict]) -> List[str]:
        """Categorize the overall content type"""
        categories = []
        
        # Combine all analysis text
        all_text = ""
        for analysis in frame_analyses:
            all_text += analysis.get('analysis', '') + " "
        for analysis in subtitle_analyses:
            all_text += analysis.get('analysis', '') + " "
        
        all_text = all_text.lower()
        
        # Category detection
        category_keywords = {
            "educational": ["learn", "teach", "explain", "demonstrate", "tutorial", "lesson", "instruction"],
            "entertainment": ["fun", "funny", "comedy", "laugh", "entertainment", "show", "performance"],
            "drama": ["dramatic", "emotion", "serious", "intense", "conflict", "tension"],
            "documentary": ["documentary", "fact", "real", "truth", "investigate", "report"],
            "commercial": ["product", "brand", "advertisement", "promotion", "buy", "sell"],
            "news": ["news", "report", "journalist", "breaking", "update", "current"],
            "sports": ["sport", "game", "play", "team", "compete", "win", "athlete"],
            "music": ["music", "song", "sing", "dance", "concert", "band", "performance"],
            "cooking": ["cook", "recipe", "food", "kitchen", "prepare", "ingredient"],
            "travel": ["travel", "trip", "visit", "journey", "explore", "destination"],
            "technology": ["technology", "computer", "software", "digital", "tech", "innovation"],
            "lifestyle": ["lifestyle", "daily", "routine", "personal", "life", "living"]
        }
        
        for category, keywords in category_keywords.items():
            score = sum(1 for keyword in keywords if keyword in all_text)
            if score >= 2:  # Require at least 2 keyword matches
                categories.append(category)
        
        return categories[:5]  # Top 5 categories
    
    async def _generate_comprehensive_analysis(self, frames: List[VideoFrame], frame_analyses: List[Dict],
                                             subtitles: List[SubtitleSegment], subtitle_analyses: List[Dict],
                                             scene_breakdown: List[Dict], visual_elements: Dict[str, Any],
                                             content_categories: List[str]) -> str:
        """Generate comprehensive overall analysis"""
        
        # Prepare comprehensive summary data
        video_duration = frames[-1].timestamp if frames else 0
        
        # Create detailed prompt
        prompt = f"""Based on comprehensive analysis of this video, provide an in-depth assessment.

VIDEO OVERVIEW:
- Duration: {video_duration:.1f} seconds
- Frames analyzed: {len(frames)}
- Subtitle segments: {len(subtitles)}
- Identified scenes: {len(scene_breakdown)}
- Content categories: {', '.join(content_categories)}

VISUAL ANALYSIS HIGHLIGHTS:
{self._format_visual_highlights(frame_analyses[:5])}

DIALOGUE ANALYSIS HIGHLIGHTS:
{self._format_dialogue_highlights(subtitle_analyses[:3])}

SCENE BREAKDOWN:
{self._format_scene_breakdown(scene_breakdown)}

VISUAL ELEMENTS SUMMARY:
- Dominant subjects: {', '.join([f"{s[0]} ({s[1]}x)" for s in visual_elements.get('dominant_subjects', [])[:3]])}
- Common objects: {', '.join([f"{o[0]} ({o[1]}x)" for o in visual_elements.get('common_objects', [])[:3]])}
- Color palette: {', '.join([f"{c[0]} ({c[1]}x)" for c in visual_elements.get('color_palette', [])[:3]])}

Please provide a comprehensive analysis covering:

1. **CONTENT OVERVIEW**: What is this video about? What is its primary purpose?

2. **NARRATIVE STRUCTURE**: How is the story or information organized? Key scenes and progression.

3. **VISUAL STYLE & PRODUCTION**: Cinematography, lighting, color usage, and overall production quality.

4. **DIALOGUE & AUDIO**: Analysis of spoken content, tone, and audio-visual synchronization.

5. **ARTISTIC & TECHNICAL MERIT**: Creative choices, technical execution, and overall craftsmanship.

6. **AUDIENCE & PURPOSE**: Target audience, intended impact, and effectiveness in achieving goals.

7. **CULTURAL & CONTEXTUAL SIGNIFICANCE**: Any cultural references, historical context, or broader implications.

8. **STRENGTHS & AREAS FOR IMPROVEMENT**: What works well and what could be enhanced.

Be thorough, analytical, and provide specific examples from the content analyzed."""

        try:
            # Rate limiting for comprehensive analysis
            await self.api_manager.acquire() 
            
            result = await self.fireworks_client.analyze_text(
                text=prompt,
                model_type="gpt_oss",
                max_tokens=800  # More tokens for comprehensive analysis
            )
            return result.content
            
        except Exception as e:
            logger.error(f"Comprehensive analysis failed: {e}")
            return self._generate_fallback_analysis(frames, frame_analyses, subtitles, content_categories)
    
    def _format_visual_highlights(self, frame_analyses: List[Dict]) -> str:
        """Format visual highlights for the comprehensive prompt"""
        highlights = []
        for analysis in frame_analyses:
            if analysis.get('analysis') and not 'failed' in analysis.get('analysis', '').lower():
                highlights.append(f"- {analysis['timestamp']:.1f}s: {analysis['analysis'][:150]}...")
        return "\n".join(highlights) if highlights else "No detailed visual analysis available"
    
    def _format_dialogue_highlights(self, subtitle_analyses: List[Dict]) -> str:
        """Format dialogue highlights for the comprehensive prompt"""
        highlights = []
        for analysis in subtitle_analyses:
            if analysis.get('analysis') and not 'failed' in analysis.get('analysis', '').lower():
                highlights.append(f"- {analysis['subtitle_range']}: {analysis['analysis'][:150]}...")
        return "\n".join(highlights) if highlights else "No dialogue analysis available"
    
    def _format_scene_breakdown(self, scene_breakdown: List[Dict]) -> str:
        """Format scene breakdown for the comprehensive prompt"""
        breakdown = []
        for scene in scene_breakdown:
            breakdown.append(f"- Scene {scene['scene_number']} ({scene['start_time']:.1f}s-{scene['end_time']:.1f}s): {scene['summary'][:100]}...")
        return "\n".join(breakdown) if breakdown else "No scene breakdown available"
    
    def _generate_fallback_analysis(self, frames: List[VideoFrame], frame_analyses: List[Dict], 
                                   subtitles: List[SubtitleSegment], content_categories: List[str]) -> str:
        """Generate fallback analysis if comprehensive analysis fails"""
        duration = frames[-1].timestamp if frames else 0
        
        return f"""COMPREHENSIVE VIDEO ANALYSIS

This {duration:.1f}-second video has been analyzed across {len(frames)} frames and {len(subtitles)} subtitle segments.

CONTENT TYPE: The video appears to be {', '.join(content_categories[:3]) if content_categories else 'general content'}.

VISUAL CONTENT: The analysis covers multiple scenes with varying visual elements including different subjects, objects, and environmental settings. The production demonstrates specific cinematographic choices in framing, lighting, and composition.

DIALOGUE & AUDIO: {'Dialogue analysis reveals conversational content with emotional and informational elements.' if subtitles else 'No dialogue content was available for analysis.'}

TECHNICAL QUALITY: The video demonstrates professional production values with attention to visual composition and technical execution.

OVERALL ASSESSMENT: This video presents a cohesive narrative or informational structure with clear visual and audio elements designed to engage its intended audience. The analysis reveals careful consideration of both technical and creative aspects in its production."""

    def save_enhanced_results(self, results: EnhancedVideoAnalysisResult, output_path: str = "enhanced_analysis_results.json"):
        """Save enhanced analysis results to JSON file"""
        results_dict = asdict(results)
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(results_dict, f, indent=2)
        
        logger.info(f"‚úÖ Enhanced results saved to {output_path}")
    
    def print_enhanced_summary(self, results: EnhancedVideoAnalysisResult):
        """Print enhanced summary of the analysis"""
        print("\n" + "="*80)
        print("ENHANCED DESCRIPTIVE VIDEO ANALYSIS SUMMARY")
        print("="*80)
        print(f"Video: {results.video_path}")
        print(f"Frames analyzed: {results.frame_count}")
        print(f"Subtitles processed: {results.subtitle_count}")
        print(f"Scenes identified: {len(results.scene_breakdown)}")
        print(f"Content categories: {', '.join(results.content_categories)}")
        print(f"Processing time: {results.processing_time:.2f} seconds")
        print(f"Total cost: ${results.total_cost:.4f}")
        
        print("\n" + "-"*50)
        print("VISUAL ELEMENTS SUMMARY:")
        print("-"*50)
        
        ve = results.visual_elements
        if ve.get('dominant_subjects'):
            print(f"Main subjects: {', '.join([f'{s[0]} ({s[1]}x)' for s in ve['dominant_subjects'][:3]])}")
        if ve.get('common_objects'):
            print(f"Common objects: {', '.join([f'{o[0]} ({o[1]}x)' for o in ve['common_objects'][:3]])}")
        if ve.get('color_palette'):
            print(f"Color palette: {', '.join([f'{c[0]} ({c[1]}x)' for c in ve['color_palette'][:3]])}")
        
        print("\n" + "-"*50)
        print("SCENE BREAKDOWN:")
        print("-"*50)
        for scene in results.scene_breakdown:
            print(f"Scene {scene['scene_number']} ({scene['start_time']:.1f}s - {scene['end_time']:.1f}s):")
            print(f"  {scene['summary']}")
        
        print("\n" + "-"*50)
        print("COMPREHENSIVE ANALYSIS:")
        print("-"*50)
        print(results.overall_analysis)
        print("="*80)


# Enhanced integration with existing pipeline
class DescriptiveAnalysisPipeline:
    """Pipeline that integrates enhanced descriptive analysis with vector search"""
    
    def __init__(self, enable_vector_search: bool = True, analysis_depth: str = "comprehensive"):
        """
        Initialize enhanced pipeline
        
        Args:
            enable_vector_search: Whether to enable vector search
            analysis_depth: "basic", "detailed", or "comprehensive"
        """
        self.enable_vector_search = enable_vector_search
        self.analysis_depth = analysis_depth
        
        # Initialize enhanced analysis system
        self.enhanced_analysis = EnhancedVideoAnalysisSystem()
        
        # Initialize vector search if enabled
        self.search_interface = None
        if enable_vector_search:
            try:
                from vector_search_system import VideoSearchInterface
                self.search_interface = VideoSearchInterface()
                logger.info("‚úÖ Vector search enabled with enhanced descriptions")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Vector search disabled: {e}")
                self.enable_vector_search = False
    
    async def analyze_video_enhanced(self,
                                   video_path: str,
                                   subtitle_path: Optional[str] = None,
                                   max_frames: int = 10,
                                   fps_extract: float = 0.2,
                                   output_dir: str = "enhanced_analysis_output") -> Dict[str, Any]:
        """
        Run enhanced descriptive analysis with optional vector search indexing
        """
        
        # Create output directory
        output_path = Path(output_dir)
        output_path.mkdir(exist_ok=True)
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        print("\n" + "="*80)
        print(f"üé¨ ENHANCED DESCRIPTIVE VIDEO ANALYSIS - {self.analysis_depth.upper()} MODE")
        print("="*80)
        print(f"Video: {video_path}")
        print(f"Analysis depth: {self.analysis_depth}")
        print(f"Vector search: {'Enabled' if self.enable_vector_search else 'Disabled'}")
        print("="*80)
        
        # Step 1: Enhanced video analysis
        print("\nüìä PHASE 1: ENHANCED DESCRIPTIVE ANALYSIS")
        print("-"*50)
        
        try:
            enhanced_results = await self.enhanced_analysis.analyze_video_descriptive(
                video_path=video_path,
                subtitle_path=subtitle_path,
                max_frames=max_frames,
                fps_extract=fps_extract,
                analysis_depth=self.analysis_depth
            )
            
            # Save enhanced results
            enhanced_file = output_path / f"enhanced_analysis_{timestamp}.json"
            self.enhanced_analysis.save_enhanced_results(enhanced_results, str(enhanced_file))
            
            # Print enhanced summary
            self.enhanced_analysis.print_enhanced_summary(enhanced_results)
            
            print(f"\n‚úÖ Enhanced analysis saved to: {enhanced_file}")
            
        except Exception as e:
            logger.error(f"Enhanced analysis failed: {e}")
            raise
        
        # Step 2: Vector search indexing with enhanced descriptions
        search_status = "Not enabled"
        if self.enable_vector_search and self.search_interface:
            print("\nüîç PHASE 2: VECTOR SEARCH INDEXING (Enhanced Descriptions)")
            print("-"*50)
            
            try:
                # Convert enhanced results to dict for indexing
                video_data = {
                    "video_path": enhanced_results.video_path,
                    "frame_count": enhanced_results.frame_count,
                    "subtitle_count": enhanced_results.subtitle_count,
                    "frame_analyses": enhanced_results.frame_analyses,
                    "subtitle_analyses": enhanced_results.subtitle_analyses,
                    "overall_analysis": enhanced_results.overall_analysis
                }
                
                search_status = self.search_interface.add_video_to_search(video_data)
                print(search_status)
                print("üöÄ Enhanced descriptions will provide better search results!")
                
            except Exception as e:
                logger.error(f"Vector indexing failed: {e}")
                search_status = f"Failed: {e}"
        
        # Step 3: Generate enhanced report
        print("\nüìÑ PHASE 3: GENERATING ENHANCED REPORT")
        print("-"*50)
        
        try:
            report = {
                "timestamp": timestamp,
                "video_path": video_path,
                "analysis_depth": self.analysis_depth,
                "enhanced_features": {
                    "comprehensive_frame_analysis": True,
                    "scene_breakdown": len(enhanced_results.scene_breakdown),
                    "visual_elements_extracted": bool(enhanced_results.visual_elements),
                    "content_categorization": enhanced_results.content_categories,
                    "detailed_cinematography": self.analysis_depth == "comprehensive"
                },
                "analysis_summary": {
                    "frames_analyzed": enhanced_results.frame_count,
                    "scenes_identified": len(enhanced_results.scene_breakdown),
                    "content_categories": enhanced_results.content_categories,
                    "processing_time": enhanced_results.processing_time,
                    "analysis_cost": enhanced_results.total_cost
                },
                "vector_search": {
                    "enabled": self.enable_vector_search,
                    "indexing_status": search_status,
                    "enhanced_descriptions": True
                },
                "key_insights": {
                    "visual_highlights": [
                        f"Frame {fa['timestamp']:.1f}s: {fa['analysis'][:200]}..."
                        for fa in enhanced_results.frame_analyses[:3]
                        if fa['analysis'] and not 'failed' in fa['analysis'].lower()
                    ],
                    "scene_summaries": [
                        f"Scene {scene['scene_number']} ({scene['start_time']:.1f}s-{scene['end_time']:.1f}s): {scene['summary']}"
                        for scene in enhanced_results.scene_breakdown[:3]
                    ],
                    "visual_elements": enhanced_results.visual_elements,
                    "comprehensive_assessment": enhanced_results.overall_analysis
                },
                "search_optimization": {
                    "enhanced_descriptions": "Frame analyses include comprehensive visual, technical, and narrative details",
                    "better_search_results": "Detailed descriptions enable more accurate semantic search",
                    "example_queries": [
                        "close-up shot of person",
                        "outdoor daylight scene",
                        "dramatic lighting",
                        "conversation between characters",
                        "technical equipment visible",
                        "specific color schemes",
                        "emotional expressions",
                        "architectural elements"
                    ] if self.enable_vector_search else []
                },
                "total_cost": enhanced_results.total_cost
            }
            
            # Save enhanced report
            report_file = output_path / f"enhanced_report_{timestamp}.json"
            with open(report_file, 'w', encoding='utf-8') as f:
                json.dump(report, f, indent=2)
            
            # Create enhanced text report
            text_report_file = output_path / f"enhanced_report_{timestamp}.txt"
            with open(text_report_file, 'w', encoding='utf-8') as f:
                f.write("ENHANCED DESCRIPTIVE VIDEO ANALYSIS REPORT\n")
                f.write("="*80 + "\n\n")
                f.write(f"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
                f.write(f"Video: {video_path}\n")
                f.write(f"Analysis Depth: {self.analysis_depth.upper()}\n\n")
                
                f.write("ENHANCED FEATURES\n")
                f.write("-"*40 + "\n")
                f.write("‚úÖ Comprehensive cinematography analysis\n")
                f.write("‚úÖ Detailed visual composition breakdown\n")
                f.write("‚úÖ Scene-by-scene narrative structure\n")
                f.write("‚úÖ Visual elements categorization\n")
                f.write("‚úÖ Enhanced content categorization\n")
                if self.enable_vector_search:
                    f.write("‚úÖ Optimized for semantic search\n")
                f.write("\n")
                
                f.write("ANALYSIS SUMMARY\n")
                f.write("-"*40 + "\n")
                f.write(f"Frames analyzed: {enhanced_results.frame_count}\n")
                f.write(f"Scenes identified: {len(enhanced_results.scene_breakdown)}\n")
                f.write(f"Content categories: {', '.join(enhanced_results.content_categories)}\n")
                f.write(f"Processing time: {enhanced_results.processing_time:.2f} seconds\n\n")
                
                f.write("SCENE BREAKDOWN\n")
                f.write("-"*40 + "\n")
                for scene in enhanced_results.scene_breakdown:
                    f.write(f"Scene {scene['scene_number']} ({scene['start_time']:.1f}s - {scene['end_time']:.1f}s):\n")
                    f.write(f"  {scene['summary']}\n")
                    f.write(f"  Key elements: {', '.join(scene.get('key_visual_elements', [])[:5])}\n\n")
                
                f.write("COMPREHENSIVE ANALYSIS\n")
                f.write("-"*40 + "\n")
                f.write(f"{enhanced_results.overall_analysis}\n\n")
                
                if self.enable_vector_search:
                    f.write("ENHANCED SEARCH CAPABILITIES\n")
                    f.write("-"*40 + "\n")
                    f.write("The enhanced descriptions enable more precise semantic search.\n")
                    f.write("Example searches that work better with enhanced analysis:\n")
                    for query in report["search_optimization"]["example_queries"][:8]:
                        f.write(f"  - \"{query}\"\n")
                    f.write("\n")
                
                f.write(f"Total cost: ${report['total_cost']:.4f}\n")
            
            print(f"‚úÖ Enhanced report saved to: {report_file}")
            print(f"‚úÖ Text report saved to: {text_report_file}")
            
        except Exception as e:
            logger.error(f"Enhanced report generation failed: {e}")
            raise
        
        print("\n" + "="*80)
        print("‚úÖ ENHANCED DESCRIPTIVE ANALYSIS COMPLETE!")
        print(f"üìÅ All outputs saved to: {output_dir}")
        print(f"üí∞ Total cost: ${report['total_cost']:.4f}")
        print("üéØ Enhanced descriptions provide:")
        print("   ‚Ä¢ Detailed cinematography analysis")
        print("   ‚Ä¢ Comprehensive visual breakdowns") 
        print("   ‚Ä¢ Scene-by-scene narrative structure")
        print("   ‚Ä¢ Better semantic search results")
        print("="*80)
        
        return report


# Usage functions for easy integration
async def analyze_video_highly_descriptive(
    video_path: str,
    subtitle_path: str = None,
    max_frames: int = 10,
    fps_extract: float = 0.2,
    analysis_depth: str = "comprehensive",
    enable_vector_search: bool = True,
    output_dir: str = "enhanced_analysis_output"
):
    """
    Analyze video with highly descriptive analysis
    
    Args:
        video_path: Path to video file
        subtitle_path: Path to subtitle file (optional)
        max_frames: Maximum frames to analyze
        fps_extract: Frame extraction rate
        analysis_depth: "basic", "detailed", or "comprehensive"
        enable_vector_search: Enable vector search indexing
        output_dir: Output directory
    
    Returns:
        Enhanced analysis results
    """
    
    pipeline = DescriptiveAnalysisPipeline(
        enable_vector_search=enable_vector_search,
        analysis_depth=analysis_depth
    )
    
    return await pipeline.analyze_video_enhanced(
        video_path=video_path,
        subtitle_path=subtitle_path,
        max_frames=max_frames,
        fps_extract=fps_extract,
        output_dir=output_dir
    )


# Demo function
async def demo_enhanced_analysis():
    """Demonstrate enhanced descriptive analysis"""
    
    print("="*80)
    print("ENHANCED DESCRIPTIVE VIDEO ANALYSIS DEMO")
    print("="*80)
    
    # Test with comprehensive analysis
    results = await analyze_video_highly_descriptive(
        video_path="real_videos/vid2.mp4",
        subtitle_path="real_videos/vid2.srt",
        max_frames=5,  # Smaller number for demo
        analysis_depth="comprehensive",
        enable_vector_search=True
    )
    
    print("\nüéâ Enhanced analysis complete!")
    print("Try searching with detailed queries like:")
    print("  - 'person wearing blue shirt in close-up shot'")
    print("  - 'outdoor scene with natural lighting'") 
    print("  - 'conversation between two people indoors'")
    
    return results


if __name__ == "__main__":
    asyncio.run(demo_enhanced_analysis())